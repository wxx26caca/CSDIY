# 软件测试 52 讲

## 测试基础知识篇

### 01 | 你真的懂测试吗？从 “用户登录” 测试谈起

作为测试工程师，目标是要保证系统在各种应用场景下的功能是符合设计要求的，所以需要考虑的测试用例要多且全面。

黑盒测试方法：

- 等价类划分法，将所有可能的输入数据划分为若干个子集，在每个子集中，如果任意一个输入数据对于揭露程序中潜在的错误都有同等效果，那么这样的子集就构成了一个等价类。
- 边界值分析法，选取输入、输出的边界值进行测试。

针对 “用户登录” 功能，设计测试用例：

- 最基本的测试用例举例
    - 输入已经注册的用户名和密码，验证是否登录成功；
    - 输入已经注册的用户名和错误的密码，验证是否登录失败，提示信息是否正确；
    - 输入未注册的用户名和密码，验证是否登录失败，提示信息是否正确；
    - 用户名和密码都为空，验证是否登录失败，提示信息是否正确；
    - 用户名和密码两者之一为空，验证是否登录失败，提示信息是否正确；
    - 如果登录功能启用了验证码功能，在用户名和密码都正确的前提下，输入正确验证码，验证是否登录成功；
    - 如果登录功能启动了验证码功能，在用户名和密码都正确的前提下，输入错误验证码，验证是否登录失败，提示信息是否正确；
- 进一步扩充的测试用例：
    - 用户名和密码是否大小写敏感；
    - 前端页面是否根据设计要求限制用户名和密码长度；
    - 页面上的密码框是否加密显示；
    - 忘记用户名和密码功能是否可用；
    - 如果登录功能需要验证码，点击验证码图片是否可以更换验证码，更换后的验证码是否可用；
    - 刷新页面是否会刷新验证码；
    - 验证码有时效性的话，要验证在时效内和时效外的有效性；
    - 后台系统创建的用户第一次登录成功后，是否提示修改密码；
    - 用户登录成功，但是会话超时后，继续操作会不会重定向到用户登录界面；
    - 不同级别的用户，如 admin 用户和普通用户，登录后的权限是否正确；
    - 页面的默认焦点是否定位在用户名输入框中；
    - 是否快捷键 Tab，Enter 等可以正常使用；
- 加入除了上述**显式功能性需求**之外，其它的**非功能性测试需求**（**安全性，性能以及兼容性**）等方面的
    - 安全性测试用例（用户密码，用户输入，同时登录...）
        - 用户密码后台存储是否加密；
        - 用户密码网络传输过程中是否加密；
        - 用户密码是否有有效期，有效期到期后，是否提示修改密码；
        - 密码框是否支持复制，粘贴；
        - 密码框输入的密码是否可以在页面源码模式下被查看；
        - 用户名和密码输入框分别输入典型的 “SQL 注入攻击” 字符串，验证系统的返回页面；
        - 用户名和密码输入框分别输入典型的 “XXS 跨站脚本攻击” 字符串，验证系统行为是否被篡改；
        - 不登录的情况下，直接输入登录后的 URL 地址，验证是否会重定向到用户登录界面；
        - 连续多次登录失败的情况下，系统是否会阻止后续的尝试以应对暴力破解；
        - 同一用户在同一终端的多种浏览器上登录，验证登录功能的互斥性是否符合设计预期；
        - 同一用户先后在多台终端的浏览器上登录，验证登录是否具有互斥性；
    - 性能压力测试
        - 单用户登录的响应时间是否小于几秒（eg：3 秒）；
        - 单用户登录时，后台请求数量是否过多；
        - 高并发情况下，用户登录的响应时间是否小于几秒（eg：5 秒）；
        - 高并发情况下，服务端的监控指标是否符合预期；
        - 长时间大量用户连续登录和登出，服务器端是否会存在内存泄漏；
        - 高集合点并发场景下，是否存在资源死锁和不合理的资源等待；
    - 兼容性测试
        - 不同浏览器，验证登录页面显示以及功能正确性；
        - 相同浏览器不同版本下，验证登录页面显示以及功能正确性；
        - 不同移动设备下不同浏览器，验证登录页面显示以及功能正确性；
        - 不同分辨率的界面下，验证登录页面显示以及功能正确性；

软件测试的用例是不可穷尽的，实际中要根据时间成本和经济成本做出一定的取舍。

### 02 | 如何设计一个 "好的" 测试用例？

所谓 “好的” 测试用例，一定是能**覆盖所有的等价类以及各种边界值**的，跟是否发现缺陷无关。

- 整体完备性：一定是一个完备的整体，是**有效测试用例组成的集合**，能够完全覆盖测试需求。
- 等价类划分准确性：对于每个等价类都能保证只要其中一个输入测试通过，其他输入也一定测试通过。
- 等价类集合完备性：保证所有可能的边界值和边界条件都已经正确识别。

对大多数的软件测试而言，综合使用**等价类划分**、**边界值分析**和**错误推测**这三大类方法就足够了。

- 等价类划分
    - 有效等价类
    - 无效等价类
- 边界值分析法（通常选取正好等于，刚刚大于或刚刚小于边界的值作为测试数据）
- 错误推测方法（依赖个人能力以及过往经验），（与 “探索式测试方法” 的基本思想和理念一样）
    - **建立常见的缺陷知识库**，测试设计的过程中，使用缺陷知识库作为检查点，优化补充测试用例的设计
    - 以缺陷知识库作为**数据驱动测试**的输入来自动生成部分的测试数据

开发阶段的**单元测试**、模块集成阶段的代码级集成测试、打包部署后面向终端的 **GUI 测试**，服务器端基于 API 测试、中间件测试、前端 GUI 测试等。

从软件功能需求出发，全面地、无遗漏地识别出测试需求是至关重要的，这将直接关系到用例的测试覆盖率。

对于识别出的每个测试需求点，需要综合运用等价类划分、边界值分析和错误推测方法来全面地设计测试用例。

只有**深入理解被测试软件的架构**，你才能设计出“有的放矢”的测试用例集，去发现系统边界以及系统集成上的潜在缺陷。

必须**深入理解被测软件的设计与实现细节**，**深入理解软件内部的处理逻辑**。

需要引入**需求覆盖率**和**代码覆盖率**来衡量测试执行的完备性，并以此为依据来找出遗漏的测试点。

### 03 | 单元测试

单元测试是指，对软件中的**最小可测试单元**在与程序其他部分**相隔离**的情况下进行**检查和验证**的工作，这里的最小可测试单元**通常是指函数或者类**。

单元**测试的对象是代码**，以及**代码的基本特征**和**产生错误的原因**，然后你必须掌握单元测试的基本方法和主要技术手段，比如什么是驱动代码、桩代码和 Mock 代码等。

- 代码的基本特征与产生错误的原因

    条件分支、循环处理和函数调用等最基本的逻辑控制。

    所有的代码都是在对数据进行**分类处理**，每一次**条件判定**都是一次分类处理，嵌套的条件判定或者循环执行，也是在做分类处理。

    要做到代码功能逻辑正确，必须做到**分类正确**并且**完备无遗漏**，同时每个分类的**处理逻辑必须正确**。

- 单元测试用例详解

    一般来说，单元测试的用例是一个 “输入数据” 和 “预计输出” 的集合。

    完整的单元测试 “输入数据”：

    - 被测试函数的输入参数；
    - 被测试函数内部需要读取的全局静态变量；
    - 被测试函数内部需要读取的成员变量；
    - 函数内部调用子函数获得的数据；
    - 函数内部调用子函数改写的数据；
    - 嵌入式系统中，在中断调用时改写的数据；
    - ...

    “预计输出” 绝对不是只有函数返回值这么简单，还应该包括函数执行完成后**所改写的所有数据**：

    - 被测试函数的返回值；
    - 被测试函数的输出参数；
    - 被测试函数所改写的成员变量；
    - 被测试函数所改写的全局变量；
    - 被测试函数中进行的文件更新；
    - 被测试函数中进行的数据库更新；
    - 被测试函数中进行的消息队列更新；
    - ...

- 驱动代码，桩代码和 Mock 代码

    驱动代码是用来调用被测函数的，而桩代码和 Mock 代码是用来代替被测函数调用的真实代码的。

    ![](https://static001.geekbang.org/resource/image/4b/2f/4b593086d9370bea9afc2d12219a0c2f.png)

    驱动代码（Driver）指调用被测函数的代码，在单元测试过程中，驱动模块通常包括调用被测函数前的**数据准备**、**调用被测函数**以及**验证相关结果**三个步骤。驱动代码的结构，通常由单元测试的框架决定。

    桩代码（Stub）是用来代替真实代码的临时代码。（被测函数 A 内部调用了函数 B，但 B 函数还未实现）

    桩代码的应用首先起到了隔离和补齐的作用，使被测代码能够独立编译、链接，并独立运行。同时，桩代码还具有控制被测函数执行路径的作用。

    - 对于 Mock 代码来说，关注点是 **Mock 方法有没有被调用**，以什么样的参数被调用，被调用的次数，以及多个 Mock 函数的先后调用顺序。所以，在使用 Mock 代码的测试中，**对于结果的验证**（也就是 assert），**通常出现在 Mock 函数中**。

    - 对于桩代码来说，关注点是**利用 Stub 来控制被测函数的执行路径**，不会去关注 Stub 是否被调用以及怎么样被调用。所以，你在使用 Stub 的测试中，对于结果的验证（也就是 assert），**通常出现在驱动代码中**。
    - [Mock 代码不是桩代码](https://martinfowler.com/articles/mocksArentStubs.html)

Java 最常用的单元测试框架是 Junit 和 TestNG；C/C++ 最常用的单元测试框架是 CppTest 和 Parasoft C/C++test。

计算代码覆盖率的工具。比如 Java 的 JaCoCo，JavaScript 的 Istanbul。

需要把单元测试执行、代码覆盖率统计和持续集成流水线做集成，以确保每次代码递交，都会自动触发单元测试，并在单元测试执行过程中自动统计代码覆盖率，最后以 “单元测试通过率” 和 “代码覆盖率” 为标准来决定本次代码递交是否能够被接受。

单元测试可能的困难：

- 紧密耦合的代码难以隔离；
- 隔离后编译链接运行困难；
- 代码本身的可测试性较差，通常代码的可测试性和代码规模成正比；
- 无法通过桩代码直接模拟系统底层函数的调用；
- 代码覆盖率越往后越难提高。

### 04 | 自动化测试

什么样的项目适合自动化测试？

- 需求稳定，不会频繁变更；

- 研发和维护周期长，需要频繁执行回归测试；

    - 软件产品比软件项目更适合做自动化测试

    - 对于软件项目的自动化测试，就要看项目的具体情况了

        对比较稳定的软件功能进行自动化测试，对变动较大或者需求暂时不明确的功能进行手工测试，最终目标是用 20% 的精力去覆盖 80% 的回归测试。

- 需要在多种平台上重复运行相同测试的场景；

- 某些测试项目通过手工测试无法实现，或者手工成本太高；

- 被测软件的开发较为规范，能够保证系统的可测试性；

- 测试人员已经具备一定的编程能力。

### 05 | 软件开发各阶段的自动化测试技术

- 单元测试的自动化技术

    从广义上讲，单元测试阶段的 ”自动化“ 内涵不仅仅指测试用例执行的自动化，还应该包含以下五个方面的内容：

    - 用例框架代码生成的自动化

        举例 TestNG 框架代码应该由自动化工具生成

        ```java
        import org.testng.Assert;
        import org.testng.annotations.DataProvider;
        import org.testng.annotations.Test;
        
        public class TestNGExample {
            
            @DataProvider(name='addMethodDataProvider')
            public Object[][] dataProvider() {
                return new Object[][] {{0, 0, 0}, {0, 0, 0}, {0, 0, 0}};
            }
            
            @Test(dataProvider='addMethodDataProvider')
            public void testAddMethod(int a, int b, int result) {
                Calculator calculator = new Calculator();
                Assert.assertEquals(calculator.add(a, b), result);
            }
        }
        ```

    - 部分测试输入数据的自动化生成

        比如，某个被测函数的原型是 void fun（int* p, short b），那么测试数据自动生成技术就会为输入参数 int* p 自动生成“空”和“非空”的两个指针 p，然后分别执行函数 void fun（int* p, short b），并观察函数的执行情况。同样地，对于输入参数 short b 会自动生成超出 short 范围的 b，测试函数 fun 的行为。

    - 自动桩代码的生成

        自动桩代码的生成是指自动化工具可以对被测试代码进行扫描分析，**自动为被测函数内部调用的其他函数生成可编程的桩代码**，并提供基于测试用例的桩代码管理机制。

        用真实函数 B 代替原本桩代码函数 B 的操作，就称为“抽桩”。

    - 被测代码的自动化静态分析

        目前比较常用的代码静态分析工具有 **Sonar** 和 **Coverity** 等。

    - 测试覆盖率的自动统计与分析

        自动化工具可以自动统计各种测试覆盖率，包括代码行覆盖率、分支覆盖率、MC/DC 覆盖率等。

- 代码级集成测试的自动化技术

    将已经开发完成的软件模块放在一起测试。

    代码级集成测试中**被测函数内部调用的其他函数必须是真实的**，不允许使用桩代码代替。

    现在的开发理念追求的是系统复杂性的解耦，会去尽量避免“大单体”应用，采用 Web Service 或者 RPC 调用的方式来协作完成各个软件功能。所以现在的软件企业，尤其是互联网企业，基本不会去做代码级集成测试。

- Web Service 测试的自动化技术

    Web Service 测试，主要是指 **SOAP API** 和 **REST API** 这两类 API 测试，最典型的是采用 **SoapUI** 或者 **Postman** 等类似的工具。由于这类工具需要在界面上手动操作发起 Request 和验证 Response，难以和 CI/CD 集成，于是出现了 API 自动化测试框架。

    代码示例：

    ```java
    public void testCreateUser(data provider parameters) {
        CreateUserAPI createUserAPI = new CreateUserAPI();
        Request req = createUserAPI.buildxxxRequest(String UserId, String oldPasswd);
        Response response = req.request();
        assert(response.statusCode == 200);
    }
    
    public class CreateUserAPI extends RestAPI {
        public static String ENDPOINT = "https://xxx.com/user/create/v2/{%userId%}";
        public CreateUserAPI() {
            super(Method.PUT, ENDPOINT);
        }
        public Request buildRequest(String userId, String passwd) {
            Request req = _buildRequest();
            req.getEndpoint().addInlineParam("userId", userId);
            req.getEndpoint().addParam("oldPasswd", oldPasswd);
            return req;
        }
    }
    ```

    基于代码的 API 测试用例，通常包含三大步骤：

    - 准备 API 调用时需要的测试数据；
    - 准备 API 的调用参数并发起 API 的调用；
    - 验证 API 调用的返回结果。

    **API 自动测试框架REST Assured**。

    除了 API 测试用例执行的自动化，Web Service 测试自动化还包含以下四个方面：

    - 测试脚手架代码的自动化生成

        使得开发者更专注于测试用例的设计上。

    - 部分测试输入数据的自动生成

        针对 API 的参数和 API 调用的 Payload。

    - Response 验证的自动化

        Response 验证自动化的核心思想是**自动比较两次相同 API 调用的返回结果，并自动识别出有差异的字段值**，比较过程可以通过规则配置去掉诸如时间戳，会话 ID （Session ID）等动态值。

    - 基于 SoapUI 或者 Postman 的自动化脚本生成

        用 SoapUI 或者 Postman 初步测试验证没问题之后，转换成符合 API 测试框架规范的测试用例。

- GUI 测试的自动化技术

    基于页面元素的识别技术，对页面元素进行自动化操作，以模拟实际终端用户的行为并验证软件功能的正确性。

    - 传统 Web 浏览器

        开源 - **Selenium**

        商用 - Micro Focus 的 **UFT**

    - 移动端原生应用（Native App）

        Appium，对 iOS 环境集成了 XCUITest，对 Android 环境集成了 UIAutomator 和 Espresso。

### 06 | 测试覆盖率

- 需求覆盖率

    将**每一条分解后的软件需求和对应的测试建立一对多的映射关系**，最终目标是保证测试可以覆盖每个需求，以保证软件产品的质量。

    通常采用 **ALM**，**Doors** 和 **TestLink** 等需求管理工具来建立需求和测试的对应关系。

    需求覆盖率统计方法属于**传统瀑布模型**下的软件工程实践，传统瀑布模型**追求自上而下地制定计划、分析需求、设计软件、编写代码、测试和运维等**，在**流程上是重量级**的，已经很难适应当今互联网时代下的敏捷开发实践。

- 代码覆盖率

    至少被执行了一次的条目数占整个条目数的百分比。

    - 行覆盖率（语句覆盖率）

        指已经被执行到的语句占总可执行语句（不包含类似 C++ 的头文件声明、代码注释、空行等等）的百分比。

    - 判定覆盖率（分支覆盖率）

        用以度量程序中**每一个判定的分支是否都被测试到了**，即代码中每个判断的取真分支和取假分支是否各被覆盖至少各一次。

    - 条件覆盖率

        判定中的**每个条件的可能取值至少满足一次**，度量判定中的每个条件的结果 TRUE 和 FALSE 是否都被测试到了。

    价值：

    统计代码覆盖率的根本目的是**找出潜在的遗漏测试用例，并有针对性的进行补充**，同时还可以**识别出代码中那些由于需求变更等原因造成的不可达的废弃代码**。

    提升代码覆盖率越到后面越难，性价比越低。

    局限性：

    高的代码覆盖率不一定能保证软件的质量，但是低的代码覆盖率一定不能能保证软件的质量。

- Java 代码覆盖率工具

    **JaCoCo**，可以很方便的嵌入到 Ant，Maven 中，和 Jenkins，Sonar 也有很好的集成。

    - 实现原理

        ![](https://static001.geekbang.org/resource/image/0b/d5/0b8f26275681d8d5edde60487d70e8d5.png)

        最基本的方法就是**注入（Instrumentation）**。注入就是在**被测代码中自动插入用于覆盖率统计的探针**代码，并保证探针代码不会给原有代码带来任何影响。

        Java 代码根据注入目标的不同，可以分为源代码注入和字节码注入两大类。主流的工具例如 **ASM 使用字节码注入**。

        根据注入发生的时间点又分为 On-The-Fly 模式和 Offline 模式。

        - 实现 On-The-Fly 模式，**无需修改源代码**，也**无需提前进行字节码插桩**。它适用于**支持 Java Agent** 的运行环境。主要有两种技术方案：

            开发**自定义的类装载器**（Class Loader）实现类装载策略，每次类加载前，需要在 class 文件中插入探针，早期的 Emma 就是使用这种方案实现的探针插入；

            **借助 Java Agent**，利用执行在 main() 方法之前的拦截器方法 **premain() 来插入探针**，实际使用过程中**需要在 JVM 的启动参数中添加“-javaagent”并指定用于实时字节码注入的代理程序**，这样代理程序在装载每个 class 文件前，先判断是否已经插入了探针，如果没有则需要将探针插入 class 文件中，目前主流的 **JaCoCo** 就是使用了这个方式。

        - Offline 模式也**无需修改源代码**，但是需**要在测试开始之前先对文件进行插桩**，并事先生成插过桩的 class 文件。它适用于不支持 Java Agent 的运行环境，以及无法使用自定义类装载器的场景。

            JVM 启动时不再需要使用 Java Agent 额外开启代理，缺点是**无法实时获取代码覆盖率信息，只能在系统停机时下获取**。

            **Cobertura** 就是使用 Offline 模式的典型代表。

### 07 | 软件缺陷报告如何写

- 缺陷标题
    - 首先，对“什么问题”的描述不仅要做到清晰简洁，最关键是要足够具体，切忌不能采用过于笼统的描述。描述“什么问题”的同时还必须清楚地表述发生问题时的上下文，也就是问题出现的场景。
    - 其次，标题应该尽可能描述问题本质，而避免只停留在问题的表面。
    - 最后，缺陷标题不易过长，对缺陷更详细的描述应该放在“缺陷概述”里。

- 缺陷概述

    缺陷概述的目的是，清晰简洁地描述缺陷，使开发工程师能够聚焦缺陷的本质。

- 缺陷影响

    测试工程师准确描述缺陷影响的前提是，必须对软件的应用场景以及需求有深入的理解，这也是对测试工程师业务基本功的考验。

- 环境配置

    环境配置的内容通常是按需描述，也就是说通常只描述那些重现缺陷的环境敏感信息。

- 前置条件

    前置条件是指测试步骤开始前系统应该处在的状态，其目的是减少缺陷重现步骤的描述。合理地使用前置条件可以在描述缺陷重现步骤时排除不必要的干扰，使其更有针对性。

- 缺陷重现步骤

    确保缺陷的可重现性；找到最短的重现路径，过滤掉那些非必要的步骤，避免产生不必要的干扰。

- 期望结果和实际结果

- 优先级（Priority）和严重程度（Severity）

- 变通方案（Workaround）

- 根原因分析（Root Cause Analysis）

- 附件（Attachment）

### 08 | 测试计划如何做

一份好的测试计划要包括：测试范围、测试策略、测试资源、测试进度和测试风险预估。

- 测试范围

    测试范围中需要明确“测什么”和“不测什么”。

- 测试策略

    需要明确“先测什么后测什么”和“如何来测”这两个问题。

    测试策略会要求我们明确测试的重点，以及各项测试的先后顺序。

    测试策略还需要说明，采用什么样的测试类型和测试方法。

    - 功能测试，怎么做
    - 兼容性测试，怎么做
    - 性能测试，怎么做

- 测试资源

    测试资源就是需要明确“谁来测”和“在哪里测”这两个问题。

- 测试进度

    测试进度主要描述各类测试的开始时间，所需工作量，预计完成时间，并以此为依据来建议最终产品的上线发布时间。

    行为驱动开发，就是平时我们经常说的 BDD，指的是**可以通过自然语言书写非程序员可读的测试用例，并通过 StepDef 来关联基于自然语言的步骤描述和具体的业务操作，最典型的框架就是知名“Cucumber”**。

- 测试风险评估

    在制定测试计划时，你就要预估整个测试过程中可能存在的潜在风险，以及当这些风险发生时的应对策略。

### 09 | 核心竞争力

作为测试人员，必须要深入理解业务，但是业务知识不能等同于测试能力。

测试开发岗位的核心其实是“测试”，“开发”的目的是更好地服务于测试。

- 传统测试工程师师应该具备的核心竞争力

    - 测试策略设计能力

        对于各种不同的被测软件，能够快速准确地理解需求，并在有限的时间和资源下，明确测试重点以及最适合的测试方法的能力。

        测试策略设计能力一定是需要你在大量实践的基础上潜移默化形成的。

    - 测试用例设计能力

        要想提高测试用例设计能力，你平时就要多积累，对常见的缺陷模式、典型的错误类型以及遇到过的缺陷，要不断地总结、归纳，才能逐渐形成体系化的用例设计思维。

        同时，你还可以阅读一些好的测试用例设计实例开阔思路，日后遇到类似的被测系统时，可以做到融会贯通和举一反三。

    - 快速学习能力

    - 探索性测试思维

    - 缺陷分析能力

    - 自动化测试技术

    - 良好的沟通能力

- 测试开发工程师的核心竞争力

    - 测试系统需求分析能力
    - 更宽广的知识体系

### 10 | 非测试知识

- 网站架构的核心知识

    网站高性能架构设计、网站高可用架构设计、网站伸缩性架构设计和网站可扩展性架构设计

- 容器技术

- 云计算技术

- DevOps 思维

    可以从深入掌握 Jenkins 之类的工具开始，到熟练应用和组合各种 plugin 来完成灵活高效的流水线搭建，之后再将更多的工具逐渐集成到流水线中以完成更多的任务。

- 前端开发技术

### 11 | 互联网产品的测试策略

现今的互联网产品往往采用菱形模型。菱形模型有以下四个关键点：

- 以中间层的 API 测试为重点做全面的测试。
- 轻量级的 GUI 测试，只覆盖最核心直接影响主营业务流程的 E2E 场景。
- 最上层的 GUI 测试通常利用探索式测试思维，以人工测试的方式发现尽可能多的潜在问题。
- 单元测试采用“分而治之”的思想，只对那些相对稳定并且核心的服务和模块开展全面的单元测试，而应用层或者上层业务只会做少量的单元测试。

互联网产品的 GUI 测试通常采用**“手工为主，自动化为辅”**的测试策略，手工测试往往利用探索性测试思想，针对新开发或者新修改的界面功能进行测试，而自动化测试的关注点主要放在相对稳定且核心业务的基本功能验证上。所以，**GUI 的自动化测试往往只覆盖最核心且直接影响主营业务流程的 E2E 场景**。

## GUI 自动化测试篇

### 12 | 第一个 GUI 自动化测试

- 构建一个 Selenium 自动化测试用例示例

    - 测试需求：

        访问百度页面 www.baidu.com，搜索某个关键词，验证搜索结果的标题是 “被搜索关键词” + “_百度搜索”。

    - 手工执行测试步骤：

        1. 打开 Chrome 浏览器，输入百度网址 www.baidu.com
        2. 在搜索框中输入关键词 “极客时间”，并按下回车
        3. 验证搜索结果页面的标题是否是 “极客时间_百度搜索”

    - Selenium 自动化测试脚本

        - 下载 Chrome Driver，加入环境变量，建一个空的 Maven 项目，在 POM 中加入 Selenium 2.0 依赖

        - 创建一个 main 方法，加入代码

            ```java
            import org.junit.Assert;
            import org.openqa.selenium.By;
            import org.openqa.selenium.WebDriver;
            import org.openqa.selenium.WebElement;
            import org.openqa.selenium.chrome.ChromeDriver;
            
            public class seleniumBaiduExample {
                public static void main(String[] args) throws InteruptedException {
                    WebDriver driver = new ChromeDriver();
                    driver.navigate().to("http://www.baidu.com");
                    WebElement search_input = driver.findElement(By.name('wd'));
                    search_input.sendKeys("极客时间");
                    search_input.submit();
                    Thread.sleep(300);
                    Assert.assertEquals("极客时间_百度搜索", driver.getTitle());
                    driver.quit();
                }
            }
            ```

- Selenium 的实现原理

    - Selenium 1.0 的工作原理

        Selenium 1.0 的核心是 **Selenium RC**（Selenium Remote Control），它的原理是：**JavaScript 代码可以很方便地获取页面上的任何元素并执行各种操作**。

        ![](https://static001.geekbang.org/resource/image/30/b6/30bb6d776cd499b727d83fa4499ca9b6.png)

        Remote Control Server 包括 Selenium Core，Lanucher，Http proxy

        - Selenium Core，是被注入到浏览器页面中的 JavaScript 函数集合，用来实现界面元素的识别和操作

        - Http Proxy，作为代理服务器修改 JavaScript 的源，以达到“欺骗”被测站点的目的；

        - Launcher，用来在启动测试浏览器时完成 Selenium Core 的注入和浏览器代理的设置。

        Client Libraries，是测试用例代码**向 Selenium RC Server 发送 http 请求的接口**，支持多种语言。

        Selenium RC 的执行流程

        ![](https://static001.geekbang.org/resource/image/9c/69/9c8317d792a8798b3f2cdedf80ff2e69.png)

        - 基于不同语言的 Client Libraries 向 Selenium RC Server 发送 http 请求，要求与其建立连接。
        - 建立连接后，Lanucher 会启动浏览器或者重用之前的浏览器，把 Selenium Core 加载到浏览器页面，并同时把浏览器的代理设置为 http proxy。
        - 测试用例通过 Client Libraries 向 Selenium RC Server 发送 http 请求，Selenium RC Server 解析之后，通过 http proxy 发送  JS 命令通知 Selenium Core 执行浏览器上控件的具体操作。
        - Selenium Core 接收到指令后，执行操作。
        - 如果浏览器收到新的页面请求信息，则会发送 Http 请求来请求新的 Web 页面。由于 Launcher 在启动浏览器时把 Http Proxy 设置成为了浏览器的代理，所以 Selenium RC Server 会接收到所有由它启动的浏览器发送的请求。
        - Selenium RC Server 接收到浏览器发送的 Http 请求后，重组 Http 请求以规避“同源策略”，然后获取对应的 Web 页面。
        - Http Proxy 把接收的 Web 页面返回给浏览器，浏览器对接收的页面进行渲染。

    - Selenium 2.0 的工作原理

        Selenium 2.0 是基于 WebDriver 实现页面操作的。

        执行流程：

        ![](https://static001.geekbang.org/resource/image/55/26/5536bccaa266329c324fa9033ee16826.png)

        - 绑定。

            Selenium 2.0 启动 web browser 的时候，后台会同时启动基于 WebDriver Wire 协议的 Web Service 作为 Selenium 的 Remote Server，并将其与浏览器绑定。Remote Server 开始监听 Client 端的操作请求。

        - 测试用例发送请求

            该 HTTP Request 的 body，是以 WebDriver Wire 协议规定的 JSON 格式来描述需要浏览器执行的具体操作。

        - 解析请求，并发送给 WebDriver

        - WebDriver 直接操作浏览器

### 13 | 脚本与数据的解耦 + Page Object 模型

- 数据驱动测试

    将数据与测试脚本解耦，很好的解决了大量重复脚本的问题。

    数据文件中不仅可以包含测试输入数据，还可以包含测试验证结果数据，甚至可以包含测试逻辑分支的控制变量。

- Page Object 模型

    利用模块化思想，把**一些通用的操作集合打包成一个个名字有意义的操作函数**，然后 GUI 自动化脚本直接去调用这些操作函数来构成整个测试用例。可以解决早期自动化 GUI 测试记流水账形式的 “可读性差，难以维护” 的问题。

    页面对象（Page Object）模型，以**页面为单位来封装页面上的控件以及控件的部分操作**。测试用例，基于页面封装对象来完成具体的界面操作，最典型的模式是 “XXXPage.YYYComponent.ZZZOperation”

### 14 | 让自动化测试脚本更好地描述业务

问题：引入操作函数封装时，**如何把控操作函数的粒度**，**如何衔接两个操作函数之间的页面**。

- 如何把控操作函数的粒度

    一个操作函数到底应该包含多少操作步骤才是最合适的？

    很大程度上取决于项目的实际情况，测试用例步骤的设计。

- 如何衔接两个操作函数之间的页面

    **前序操作函数完成后的最后一个页面，必须是后续操作函数的第一个页面**。

    如果连续的两个操作函数之间无法用页面衔接，那就需要在两个操作函数之间加入额外的页面跳转代码，或者是在操作函数内部加入特定的页面跳转代码。

业务流程抽象

业务流程抽象是，**基于操作函数的更接近于实际业务的更高层次的抽象方式**。基于业务流程抽象实现的测试用例往往灵活性会非常好，可以很方便地组装出各种测试用例。

例子：已注册的用户登录电商平台购买指定的书籍。

```java
// 伪代码
// Business Flow - login flow
LoginFlowParameters loginFlowParameters = new LoginFlowParameters();
loginFlowParameters.setUserName("username");
loginFlowParameters.setPassword("password");
LoginFlow loginFlow = new LoginFlow(loginFlowParameters);
loginFlow.execute();

// Business Flow - Search book flow
SearchBookFlowParameters searchBookFlowParameters = new SearchBookFlowParameters();
searchBookFlowParameters.setBookName("bookname");
SearchBookFlow searchBookFlow = new SearchBookFlow(searchBookFlowParameters);
// 很方便地完成两个业务流程之间的页面衔接。
searchBookFlow.withStartPage(loginFlow.getEndPage()).execute();


// Business Flow - Checkout/buy book flow
CheckoutBookFlowParameters checkoutBookFlowParameters = new CheckoutBookFlowParameters();
checkoutBookFlowParameters.setBookID(searchBookFlow.getOutPut().getBookID());
CheckoutBookFlow checkoutBookFlow = new CheckoutBookFlow(checkoutBookFlowParameters);
checkoutBookFlow.withStartPage(searchBookFlow.getEndPage()).execute();

// Business Flow - logout flow
LogoutFlow logoutFlow = new LogoutFlow();
logoutFlow.witchStartPage(checkoutBookFlow.getEndPage()).execute();
```

业务流程的优点：

- 业务流程（Business Flow）的封装更接近实际业务；

- 基于业务流程的测试用例非常标准化，遵循 **“参数准备”、“实例化 Flow” 和 “执行 Flow”** 这三个大步骤，非常适用于测试代码的自动生成；
- 由于更接近实际业务，所以可以很方便地和 BDD （Behavior Driven Development）结合。

### 15 | GUI 自动化过程中测试数据

GUI 测试中常见的两种数据类型

- 测试输入数据

    eg: 用户登录测试中的用户名和密码

- 为了完成 GUI 测试而需要准备的测试数据

    用户登录测试中的用户

测试数据的准备：

- 创建方式：API 调用；数据库操作；综合运用两者

    实际上，往往很多测试数据的创建是基于 API 和数据库操作两者的结合来完成，即**先通过 API 创建基本的数据，然后调用数据库操作来修改数据**，以达到对测试数据的特定要求。

    对比：

    - API 调用：简单方便，但是不是所有的测试数据都有相关的 API 来支持，并发效果不理想
    - 数据库操作：可以创建和修改 API 不支持的测试数据，效率远高于 API 调用方法，但是直接操作数据库会经常出现 SQL 语句更新不及时导致测试数据错误的问题。

- 创建时机：测试过程中实时创建（On-the-fly）；事先创建好（Out-of-box）

    对于**相对稳定的测试数据**，比如商品类型、图书类型等，**往往采用 Out-of-box 的方式**以提高效率；而对于那些只能**一次性使用的测试数据**，比如商品、订单、优惠券等，**往往采用 On-the-fly 的方式**以保证不存在脏数据问题。

    对比：

    - 实时创建

        在用例执行过程中实时创建数据，导致测试的执行时间比较长。

        业务数据的连带关系，导致测试数据的创建效率非常低。

        实时创建测试数据的方式对测试环境的依赖性很强。

    - 事先创建

        测试用例中需要硬编码（hardcode）测试数据，额外引入了测试数据和用例之间的依赖。

        只能被一次性使用的测试数据不适合 Out-of-box 的方式。

        “预埋”的测试数据的可靠性远不如实时创建的数据。

### 16 | Page Code Gen + Data Gen + Headless

- 页面对象自动生成

    页面对象自动生成技术，属于典型的“自动化你的自动化”的应用场景。它的基本思路是，你不用再手工维护 Page Class 了，只需要提供 Web 的 URL，它就会自动帮你生成这个页面上所有控件的定位信息，并自动生成 Page Class。

    商用工具 **UFT**，已经支持页面对象自动生成功能了，同时还能够对 Page Class 进行版本管理。

    免费的 **Katalon Studio**，已经提供了类似的页面对象库管理功能。

- GUI 测试数据自动生成

    - 根据 GUI 输入数据类型，以及对应的**自定义规则库自动生成测试输入数据**。
    - 对于需要组合多个测试输入数据的场景，测试数据自动生成可以自动完成多个测试数据的笛卡尔积组合，然后再以人工的方式剔除掉非法的数据组合。

- 无头浏览器

    它拥有完整的浏览器内核。与普通浏览器最大的不同是，**无头浏览器执行过程中看不到运行的界面，但是你依然可以用 GUI 测试框架的截图功能截取它执行中的页面**。

    2017 年 Google 发布了 Headless Chrome，以及与之配套的 **Puppeteer** 框架，Puppeteer 不仅支持最新版本的 Chrome，而且得到 Google 官方的支持，这使得无头浏览器可以在实际项目中得到更好的应用。

    Puppeteer 是一个 Node 库，提供了高级别的 API 封装，**这些 API 会通过 Chrome DevTools Protocol 与 Headless Chrome 的交互达到自动化操作的目的**。

    目前的应用领域**主要在爬虫和 devops 中的环境健康检查**，就是去看一下网站基本的页面是否可以打开，最最基本的 smoke 用例是否可以通过，如果不行，那就会把环境健康状态标红，并从可用列表中移除

### 17 | GUI 测试稳定性的关键技术

GUI 自动化测试稳定性，最典型的表现形式就是，**同样的测试用例在同样的环境下，时而测试通过，时而测试失败**。

造成 GUI 测试不稳定的因素：

- 非预计的弹出对话框

    - **操作系统弹出的非预计对话框**。比如杀毒软件更新请求，系统更新请求等。
    - 被测软件在**非预期的时间弹出预期的对话框**。

    处理方法：当自动化脚本发现控件无法正常定位的时候，GUI 自动化框架进入 “**异常场景恢复模式**”，依次检查各种可能出现的对话框，一旦确认了对话框的类型，执行预定义的操作（比如，单击确定，或者关闭对话框），然后重试刚才的步骤。

    缺点：只能处理已知可能出现的对话框。当发现一种潜在对话框的时候，可以更新到 “异常场景恢复” 库中，下次再遇到相同类型的，就可以自动关闭了。

- 页面控件属性的细微变化

    比如某个 button 的 id 突然变了一位数字。

    处理方法：采用 “**组合属性” 定位控件**可以更精确，可以在此基础上加入 **“模糊匹配”** 技术，进一步提高控件的识别率。

    开源的 GUI 自动化测试框架，目前还没有现成的框架直接支持模糊匹配，通常需要你进行二次开发，实现思路是：实现自己的对象识别控制层，也就是**在原本的对象识别基础上额外封装一层，在这个额外封装的层中加上模糊匹配的实现逻辑**。

- 被测系统的 A/B 测试

    A/B 测试：为 Web 或 App 的界面或流程**提供两个不同的版本**，然后让用户**随机访问其中一个版本**，并收集两个版本的用户体验数据和业务数据，最后**分析评估出最好的版本用于正式发布**。

    处理方法：在测试脚本内部**对不同的被测版本做分支处理**，脚本需要能够区分 A 和 B 两个的不同版本，并做出相应的处理。

- 随机的页面延迟造成控件识别失败

    处理方法：加入重试（retry）机制。重试可以是步骤级别的，也可以是页面级别的，甚至是业务流程级别的。

    对于开源 GUI 测试框架，重试机制往往不是自带的功能，需要自己二次开发来实现。

    需要特别注意的是，**对于那些会修改一次性使用数据的场景**，切忌不要盲目启用页面级别和业务流程级别的重试。

- 测试数据问题

    比如，测试用例所依赖的数据被其他用例修改了；再比如，测试过程中发生错误后自动进行了重试操作，但是数据状态已经在第一次执行中被修改了。

### 18 | GUI 自动化测试报告

商业的 GUI 自动化测试软件，比如使用最为广泛的 **UFT**（就是以前的 QTP），已经自带了截图以及高亮显示操作元素功能。

如果是开源软件，比如 Selenium WebDriver，那就需要自己去实现截图以及高亮显示操作元素的功能。

实现的思路通常是：利用 Selenium WebDriver 的 **screenshot 函数**在一些**特定的时机**（比如，页面发生跳转时，在页面上操作某个控件时，或者是测试失败时，等等）完成界面截图功能。

具体到代码实现，通常有两种方式：

- 扩展 Selenium 原本的操作函数；

    - 既然 Selenium 原生的 click 操作函数并不具备截图以及高亮显示操作元素的功能，那我们就来实现一个自己 click 函数。
    - 当自己实现的 click 函数被调用时：
        - 首先，利用 JavaScript 在对象的边框上渲染一个 5-8 个像素的边缘高亮显示被操作的元素；
        - 然后，调用 screenshot 函数完成点击前的截图；
        - 最后，调用 Selenium 原生的 click 函数完成真正的点击操作。

- 在相关的 Hook 操作中调用 screenshot 函数。

    JUnit 和 TestNG，都有所谓的 BeforeTest 和 AfterTest 方法，这些都是可以在特定步骤的前后插入自定义操作的接口。

全球化 GUI 测试报告的创新设计

报告的横向，是一个国家的业务测试顺序截图，报告的纵向，展示的是同一界面在不同国家的形式。

整个报告可以用键盘上下左右依次移动。

可以在测试报告中直接提供**递交缺陷的按钮**，一旦发现问题直接递交缺陷，同时还可以把相关截图一起直接递交到缺陷管理系统，这将更大程度地提高整体效率。

### 19 | 大型项目中设计 GUI 自动化测试策略

测试策略如何设计？测试用例脚本如何组织来实现最大可重复利用？

由于大型全球化电商网站的业务极其庞大，所以前端架构也要**按照不同的业务模块来划分**，比如**用户管理模块**、**商户订单管理模块**、**商户商品管理模块**等等。

- 首先，要从前端组件的级别来保证质量，也就是需要**对那些自定义开发的组件进行完整全面的测试**。最常用的方案是：**基于 Jest 开展单元测试，并考量 JavaScript 的代码覆盖率指标**。完成单元测试后，往往还会**基于被测控件构建专用的测试页面，在页面层面再次验证控件相关的功能和状态**。
    - 先构建一个空页面，并加入被测控件，由此可以构建出一个包含被测控件的测试页面，这个页面往往被称为 Dummy Page；
    - 从黑盒的角度出发，在这个测试页面上通过手工和自动化的方式操作被测控件，并验证其功能的正确性。

- 其次，每一个前端模块，都会构建自己的页面对象库，并且在此基础上封装开发自己的业务流程脚本。这些业务流程的脚本，可以组装成每个前端模块的测试用例。（希望这个阶段的自动化率可以达到 70%-80%）

- 最后，组合各个前端模块，并站在终端用户的视角，以黑盒的方式使用网站的端到端（E2E）测试。（希望端到端的 GUI 自动化测试用例 100% 通过）

    - 一部分是，通过**探索式测试**的方法手工执行测试，目标是尽可能多地发现新问题；
    - 另一部分是，通过 **GUI 自动化测试**执行基本业务功能的回归测试，保证网站核心业务相关的所有功能的正确性。

    E2E 团队应该尽可能地利用各个模块已有的页面对象和业务流程脚本，组装端到端的 GUI 测试。

GUI 自动化测试脚本管理

将各个模块的页面对象和业务流程脚本放在各自的代码库中，并引入页面对象和业务流程脚本的版本管理机制，通常采用页面对象和业务流程脚本的版本号和开发版本号保持一致的方案。

![](https://static001.geekbang.org/resource/image/bf/49/bf72fc0dc07739f21c3ea3de30b01049.png)

### 20 | 移动应用测试方法与思路

移动端应用又可以进一步细分为三大类：Web App、Native App 和 Hybrid App。

![](https://static001.geekbang.org/resource/image/1c/67/1c3526428800a068d56dc8e194645867.png)

Web App 指的是移动端的 Web 浏览器；（跟 PC 端的差不多）

Native App 指的是移动端的原生应用；（iOS 一般采用 XCUITest Driver，而 Android 一般采用 UiAutomator2 或者 Espresso 等）

Hybrid App（俗称：混血应用），是介于 Web App 和 Native App 两者之间的一种 App 形式。（在原生移动应用中嵌入了 Webview，然后通过该 Webview 来访问网页。）（可能需要切换上下文，比如 Native Container 和 web code）

移动应用专项测试的思路和方法：

- 交叉事件测试

    交叉事件测试也叫中断测试，是指 App 执行过程中，有其他事件或者应用中断当前应用执行的测试。

    此类测试目前基本还都是采用手工测试的方式，并且都是在真机上进行，不会使用模拟器。

    交叉事件测试，需要覆盖的场景主要包括：

    - 多个 App 同时在后台运行，并交替切换至前台是否影响正常功能；
    - 要求相同系统资源的多个 App 前后台交替切换是否影响正常功能，比如两个 App 都需要播放音乐，那么两者在交替切换的过程中，播放音乐功能是否正常；
    - App 运行时接听电话；
    - App 运行时接收信息；
    - App 运行时提示系统升级；
    - App 运行时发生系统闹钟事件；
    - App 运行时进入低电量模式；
    - App 运行时第三方安全软件弹出告警；
    - App 运行时发生网络切换，比如，由 Wifi 切换到移动 4G 网络，或者从 4G 网络切换到 3G 网络等；
    - ...

- 兼容性测试

    要确保 App 在各种终端设备、各种操作系统版本、各种屏幕分辨率、各种网络环境下，功能的正确性。

    **Appium + Selenium Grid + OpenSTF** 去搭建自己的移动设备私有云平台。

    第三方的移动设备云测平台，国外最知名的是 SauceLab，国内主流的是 Testin。

    - 不同操作系统的兼容性，包括主流的 Andoird 和 iOS 版本；
    - 主流的设备分辨率下的兼容性；
    - 主流移动终端机型的兼容性；
    - 同一操作系统中，不同语言设置时的兼容性；
    - 不同网络连接下的兼容性，比如 Wifi、GPRS、EDGE、CDMA200 等；
    - 在单一设备上，与主流热门 App 的兼容性，比如微信、抖音、淘宝等；
    - ...

- 流量测试

    往往借助于 Android 和 iOS 自带的工具进行流量统计，也可以利用 tcpdump、Wireshark 和 Fiddler 等网络分析工具。

    - App 执行业务操作引起的流量；
    - App 在后台运行时的消耗流量；
    - App 安装完成后首次启动耗费的流量；
    - App 安装包本身的大小；
    - App 内购买或者升级需要的流量。

- 耗电量测试

    Android 通过 adb 命令“adb shell dumpsys battery”来获取应用的耗电量信息；

    iOS 通过 Apple 的官方工具 Sysdiagnose 来收集耗电量信息，然后，可以进一步通过 Instrument 工具链中的 Energy Diagnostics 进行耗电量分析。

    - App 运行但没有执行业务操作时的耗电量；
    - App 运行且密集执行业务操作时的耗电量；
    - App 后台运行的耗电量。

- 弱网络测试

    移动应用的测试需要保证在复杂网络环境下的质量。具体的做法就是：在测试阶段，模拟这些网络环境，在 App 发布前尽可能多地发现并修复问题。

    推荐工具 Facebook 的 Augmented Traffic Control（ATC）。

- 边界测试

    边界测试是指，移动 App 在**一些临界状态下的行为功能的验证测试**，基本思路是需要找出各种潜在的临界场景，并对每一类临界场景做验证和测试。

    - 系统内存占用大于 90% 的场景；
    - 系统存储占用大于 95% 的场景；
    - 飞行模式来回切换的场景；
    - App 不具有某些系统访问权限的场景，比如 App 由于隐私设置不能访问相册或者通讯录等；
    - 长时间使用 App，系统资源是否有异常，比如内存泄漏、过多的链接数等；
    - 出现 ANR （Application Not Response）的场景；
    - 操作系统时间早于或者晚于标准时间的场景；
    - 时区切换的场景；
    - …

### 21 | Appium

⭐⭐⭐实战

## API 自动化测试

### 22 | API 测试及常用工具

API 测试的基本步骤：

- 准备测试数据（可选）
- 通过 API 测试工具，发起对被测 API 的 request
- 验证返回结果的 response

常见的命令行工具 **cURL**、图形界面工具 **Postman** 或者 **SoapUI**、API 性能测试的 **JMeter** 等。

基于 Spring Boot 构建的 API

https://github.com/SpectoLabs/spring-cloud-contract-blog

- cURL

    > -i : 显示 response 的 header 信息
    >
    > -H : 设定 request 的 Header
    >
    > -X : GET, PUT, POST, DELETE
    >
    > -d : 加参数
    >
    > -b : 需要传递 cookie 时，指定 cookie 文件的路径

    使用场景

    - Session 场景

        ```
        curl -i -H "sessionid:xxxxxx" -X GET "http://xxx/api/demoAPI"
        ```

    - Cookie 场景

        ```
        // 将cookie保存为文件
        curl -i -X POST -d username=robin -d password=password123 -c ~/cookie.txt "http://XXX/auth"
        // 载入cookie到request中
        curl -i -H "Accept:application/json" -X GET -b ~/cookie.txt "http://XXX/api/demoAPI"
        ```

- Postman

    - 发起 API 调用

    - 添加结果验证

    - 保存测试用例

    - 基于 Postman 的测试代码自动生成

        - 将 Postman 中的测试 request 用自动化的方式直接转换成 API 测试的代码。

        - 利用 Newman 工具直接执行 Postman 导出的 Collection JSON 文件。

            ```
            newman run examples/sample-collection.json;
            ```

- 复杂场景的 API 测试

    - 场景一：被测业务操作是由多个 API 调用协作完成。

        一个单一的前端操作可能会触发后端一系列的 API 调用，必须直接从后端通过**模拟 API 的顺序调用**来模拟测试过程。

        如何才能高效地获取单个前端操作所触发的 API 调用序列？

        - 可以通过网络监控的手段，捕获单个前端操作所触发的 API 调用序列。（Fiddler）
        - 基于用户行为日志，通过大数据手段来获取这个序列。

    - 场景二：API 测试过程中的第三方依赖。

        在微服务架构下，API 间相互耦合的依赖问题会非常严重。

        解决这个问题的核心思路是，**启用 Mock Server 来代替真实的 API**。

    - 场景三：异步 API 测试。

        此处的异步 API 指的是调用后立即返回结果，但是实际的任务还没有起来。

        对异步 API 的测试主要分为两个部分：

        一是，**测试异步调用是否成功**，可以通过检查返回值和后台工作线程是否被创建来查看。

        二是，**测试异步调用的业务逻辑处理是否正确**。一般需要有访问数据库或者消息队列的能力，需要测试框架提供。

### 23 | API 自动化测试框架

- 早期的基于 Postman 的 API 测试

    问题：**如果遇到需要频繁执行的大量测试用例，显得笨拙；难以与 CI/CD 流水线集成**。

- 基于 Postman 和 Newman 的 API 测试

    问题：对于简单调用单个 API 的测试，很方便，但是**对于多个有调用顺序的 API 测试**，并且有参数传递的时候，就不太理想。

- 基于代码的 API 测试

    基于 Java 的 OkHttP 和 Unirest、基于 Python 的 http.client 和 Requests、基于 NodeJS 的 Native 和 Request 等。

    问题：**对于单个 API 测试的场景**，工作量相比 Postman 要大得多；对于单个 API 测试的场景，无法直接重用 Postman 里面已经积累的 Collection。

- 自动生成 API 测试代码

    自动生成 API 测试代码是指，**基于 Postman 的 Collection 生成基于代码的 API 测试用例**。

    问题：需要将**测试中的 assert 转换成代码**；与**自研的 API 测试框架绑定**到一起。

    可以实现一个简单的工具，输入是 Postman 中 Collection 的 JSON 文件，输出是基于自研的 API 测试框架的测试代码，同时把测试的断言一并转化为代码。

    实现过程大致可以分为以下三步：

    首先，根据自研 API 框架的代码结构**建立一个带有变量占位符的模板文件**；

    然后，通过 JSON 解析程序，按照 Collection JSON 文件的格式定义去**提取 header、method 等信息**；

    最后，用提取得到的具体值**替换之前模板文件中的变量占位符**，这样就得到了可执行的自研框架的 API 测试用例代码。

- Response 结果发生变化时的自动识别

    需要找到一个方法，既可以不对所有的 response 字段都去写 assert，又可以监测到 response 的结构以及没有写 assert 的字段值的变化。

    具体思路：

    在 API 测试框架里引入一个内建数据库（比如 MongoDB），用这个数据库**记录**每次调用的 request 和 response 的组合，当下次发送相同 request 时，API 测试框架就会自动和上次的 response 做**差异检测**，对于有变化的字段**给出告警**。

    对于 token 值、session ID、时间戳等每次 API 调用都不一样的值，可以通过规则配置设立一个“**白名单列表**”，把那些动态值的字段排除在外。

⭐ **HttpRunner**，在此类 API 测试框架的支持下，测试用例本身往往就是纯粹的配置文件了。

### 24 | 微服务模式下的 API 测试

需要解决的问题：庞大的测试用例数量，微服务之间的相互耦合。

- 单体架构（Monolithic Architecture）

    单体架构是将所有的业务场景的表示层、业务逻辑层和数据访问层放在同一个工程中，最终经过编译、打包，并部署在服务器上。

    问题：

    **灵活性差**；（即使改了一行代码，也要重新编译，打包）

    **可扩展性差**；（高并发场景下，不利于应用的横向扩展）

    **稳定性差**；（任何一个模块有问题，会导致整体的应用不可用）

    **可维护性差**；（代码的复杂性随着业务复杂性直线上升）

- 微服务架构（Microservice Architecture）

    每个服务运行在其独立的进程中，开发采用的技术栈也是独立的；

    服务间**采用轻量级通信机制进行沟通**，通常是基于 HTTP 协议的 RESTful API；

    每个服务都围绕着具体的业务进行构建，并且能够被独立开发、独立部署、独立发布；

    对运维提出了非常高的要求，促进了 CI/CD 的发展与落地。

    - 解决庞大的测试用例数量的问题（基于消费者契约的 API 测试）

        ![](https://static001.geekbang.org/resource/image/1e/86/1e9c9cb78a02e3675e612da46e2c1b86.png)

    - 解决微服务之间的相互耦合问题（Mock Service 来代替被依赖的真实 Service）

    ⭐⭐一个基于 Spring Cloud Contract 的实际代码的示例演示契约文件格式、消费者契约测试以及微服务之间解耦。

    具体的实例代码，https://github.com/SpectoLabs/spring-cloud-contract-blog，

    详细的代码解读可以参考 https://specto.io/blog/2016/11/16/spring-cloud-contract/

## 代码测试

### 25 | 代码级测试的基本理念和方法

常见的代码错误类型

- 语法特征错误（从编程语法上就能发现的错误）
- 边界行为特征错误（代码在执行过程中发生异常，崩溃或者超时）
- 经验特征错误（根据过往经验发现代码错误）
- 算法错误
- 部分算法错误（在一些特定的条件或者输入情况下，算法不能准确完成业务要求实现的功能）

代码级测试常用方法

- 静态方法，就是在**不实际执行代码**的基础上发现代码缺陷的方法，又可以进一步细分为人工静态方法和自动静态方法；
- 动态方法，通过**实际执行代码**发现代码中潜在缺陷的方法，同样可以进一步细分为人工动态方法和自动动态方法。
    - 人工静态方法，本质上通过开发人员**代码走查、结对编程、同行评审**来完成的，理论上可以发现所有的代码错误，但也因为其对“测试人员”的过渡依赖，局限性非常大；
    - 自动静态方法，主要的手段是**代码静态扫描**，可以发现语法特征错误、边界行为特征错误和经验特征错误这三类“有特征”的错误；
    - 人工动态方法，**就是传统意义上的单元测试**，是发现算法错误和部分算法错误的最佳方式；
    - 自动动态方法，其实就是**自动化的边界测试**，主要覆盖边界行为特征错误。

### 26 | 静态测试方法

人工静态方法

- 代码走查（Code Review）
- 结对编程（Pair Programming）
- 同行评审（Peer Review）

自动静态方法

⭐⭐自动静态代码扫描工具 **Sonar**

Python 中可以 **pylint** 进行检查

自动静态扫描通常都会和持续集成的流水线做绑定，最常见的应用场景是当你递交代码后，持续集成流水线就会自动触发自动静态扫描，这一功能是通过 **Jenkins 以及 Jenkins 上的 SonarQube 插件**来完成的，当你在 Jenkins 中安装了 SonarQube Plugin，并且将 SonarQube 服务器相关的配置信息加入 Plugin 之后，你就可以在 Jenkins Job 的配置中增加 Sonar 静态扫描步骤了。

### 27 | 动态测试方法

人工动态方法

无非就是用驱动代码去调用被测函数，并根据代码的功能逻辑选择必要的输入数据的组合，然后验证执行被测函数后得到的结果是否符合预期。 但是，一旦要在实际项目中开展单元测试时，你会发现有很多实际的问题需要解决。

- 单元测试用例“输入参数”的复杂性

    - 被测试函数的输入参数
    - 被测试函数内部需要读取的全局静态变量
    - 被测试函数内部需要读取的类成员变量
    - 函数内部调用子函数获得的数据
    - 函数内部调用子函数改写的数据
    - 嵌入式系统中，在中断调用中改写的数据

- 单元测试用例“预期输出”的复杂性

    - 被测函数的返回值
    - 被测函数的输出参数
    - 被测函数所改写的成员变量和全局变量
    - 被测函数中进行的文件更新、数据库更新、消息队列更新等

- 关联依赖的代码不可用

    假设**被测函数中调用了其他的函数**，那么这些被调用的其他函数就是被测函数的关联依赖代码。

    一般来讲桩函数主要有两个作用，**一个是隔离和补齐**，**另一个是实现被测函数的逻辑控制**。

## 性能测试

### 28 | 不同视角的软件性能与性能指标

- 终端用户眼中的软件性能

    主要表现为用户进行业务操作时的主观响应时间。包括了系统响应时间和前端展现时间。系统响应时间又可以细分为系统处理时间、数据库处理时间和网络传输时间；前端展现时间则取决于用户端处理能力。、

- 系统运维人员眼中的软件性能

    除了包括单个用户的响应时间外，更要关注大量用户**并发访问**时的负载，可能的更大负载情况下的**系统健康状态**，并发处理能力，部署的**系统容量**，可能的**系统瓶颈**，系统**配置层面的调优**，**数据库的调优**，长时间运行的**稳定性**和**可扩展性**。

    有些系统为了能够承载更多的并发用户，往往会牺牲等待时间而引入预期的等待机制。比如，火车票购票网站

- 软件设计开发人员眼中的软件性能

    - 算法设计

        核心算法的设计与实现是否高效；必要时，设计上是否采用 buffer 机制以提高性能，降低 I/O；是否存在潜在的内存泄露；是否存在并发环境下的线程安全问题；是否存在不合理的线程同步方式；是否存在不合理的资源竞争。

    - 架构设计

        是否可以方便地进行系统容量和性能扩展；应用集群的可扩展性是否经过测试和验证；缓存集群的可扩展性是否经过测试和验证；数据库的可扩展性是否经过测试和验证。

    - 性能最佳实践

        代码实现是否遵守开发语言的性能最佳实践；关键代码是否在白盒级别进行性能测试；是否考虑前端性能的优化；必要的时候是否采用数据压缩传输；对于既要压缩又要加密的场景，是否采用先压缩后加密的顺序。

    - 数据库相关

        数据库表设计是否高效；是否引入必要的索引；SQL 语句的执行计划是否合理；SQL 语句除了功能是否要考虑性能要求；数据库是否需要引入读写分离机制；系统冷启动后，缓存大量不命中的时候，数据库承载的压力是否超负荷。

    - 软件性能的可测试性

        是否为性能分析（Profiler）提供必要的接口支持；是否支持高并发场景下的性能打点；是否支持全链路的性能分析。

- 性能测试人员眼中的软件性能

衡量软件性能的三个最常用的指标：**并发用户数**、**响应时间**，以及**系统吞吐量**。

- 并发用户数

    对于已经上线的系统来说，往往采用**系统日志分析法获取用户行为统计和峰值并发量**等重要信息；

    对于未上线的全新系统来说，通常的做法是**参考行业中类似系统的统计信息来建模**，然后分析。

- 响应实践

    应用系统从请求发出开始，到客户端接收到最后一个字节数据所消耗的时间。

- 系统吞吐量

### 29 | 性能测试的基本方法与应用领域

当系统并发用户数较少时，系统的吞吐量也低，系统处于空闲状态，我们往往把这个阶段称为 “**空闲区间**”。

当系统整体负载并不是很大时，随着系统并发用户数的增长，系统的吞吐量也会随之呈线性增长，我们往往把这个阶段称为 “**线性增长区间**”。

随着系统并发用户数的进一步增长，**系统的处理能力逐渐趋于饱和**，因此每个用户的响应时间会逐渐变长。相应地，系统的整体吞吐量并不会随着并发用户数的增长而继续呈线性增长。我们往往把这个阶段称为系统的“**拐点**”。

随着系统并发用户数的增长，**系统处理能力达到过饱和状态**。此时，如果继续增加并发用户数，最终所有用户的响应时间会变得无限长。相应地，系统的整体吞吐量会降为零，系统处于被压垮的状态。我们往往把这个阶段称为“**过饱和区间**”。

- 后端性能测试

    通过**性能测试工具**模拟**大量的并发用户请求**，然后**获取系统性能的各项指标**，并且**验证各项指标**是否符合预期的性能需求的测试手段。

    系统级别的 CPU 占用率、内存使用率、磁盘 I/O 和网络 I/O。

    应用级别以及 JVM 级别的各类资源使用率指标。

- 前端性能测试

    通常来讲，前端性能关注的是浏览器端的**页面渲染时间**、**资源加载顺序**、**请求数量**、**前端缓存使用情况**、**资源压缩**等内容，希望借此找到页面加载过程中**比较耗时的操作和资源**，然后进行有**针对性的优化**，最终达到优化终端用户在浏览器端使用体验的目的。

    雅虎（Yahoo）前端团队总结的 7 大类 35 条前端优化规则。

    [Best Practices for Speeding Up Your Web Site](https://developer.yahoo.com/performance/rules.html?guccounter=1)

    - 减少 http 请求次数
    - 减少 DNS 查询次数
    - 避免页面跳转
    - 使用内容分发网络（CDN），使用 CDN 相当于**对静态内容做了缓存**，并把缓存内容放在网络供应商（ISP）的机房，用户根据就近原则到 ISP 机房获取这些被缓存了的静态资源，因此可以大幅提高性能；
    - Gzip 压缩传输文件，压缩可以帮助减小传输文件的大小，进而可以从网络传输时间的层面来减少响应时间；

- 代码级性能测试

    在单元测试阶段就对代码的时间性能和空间性能进行必要的测试和评估，以防止底层代码的效率问题在项目后期才被发现的尴尬。

- 压力测试

    压力测试往往被用于系统容量规划的测试。

    还有些情况，在执行压力测试时，还会故意在临界饱和状态的基础上继续施加压力，直至系统完全瘫痪，观察这个期间系统的行为；然后，逐渐减小压力，观察瘫痪的系统是否可以自愈。

- 配置测试

    主要用于观察系统在不同配置下的性能表现。

    通过性能基准测试（Benchmark）建立性能基线（Baseline）；在此基础上，调整配置；基于同样的性能基准测试，观察不同配置条件下系统性能的差异，根本目的是要**找到特定压力模式下的最佳配置**。

- 并发测试

    在同一时间，同时调用后端服务，期间观察被调用服务在并发情况下的行为表现，旨在发现诸如资源竞争、资源死锁之类的问题。

    为了达到准确控制后端服务并发数的目的，我们需要让某些并发用户到达该集合点时，先处于等待状态，直到参与该集合的全部并发用户都到达时，再一起向后端服务发起请求。（集合点并发）

- 可靠性测试

    过长时间模拟真实的系统负载来发现系统潜在的内存泄漏、链接池回收等问题。

应用领域

- 能力验证

    能力验证这个领域最常使用的测试方法，包括后端性能测试、压力测试和可靠性测试。

- 能力规划

    能力规划最常使用的测试方法，主要有后端性能测试、压力测试、可靠性测试和配置测试。

- 性能调优

- 缺陷发现

    缺陷发现，最常用的测试方法主要有后端性能测试、压力测试、并发测试和代码级性能测试。

### 30 | 后端性能测试工具原理与常用工具

- 后端性能测试和后端性能测试工具之间的关系？

    完整的后端性能测试应该包括**性能需求获取**、**性能场景设计**、**性能测试脚本开发**、**性能场景实现**、**性能测试执行**、**性能结果报告分析**、**性能优化和再验证**。

    使用性能测试工具获得性能测试报告只是性能测试过程中的一个必要步骤而已，而得出报告的目的是让性能测试工程师去做进一步的分析，以得出最终结论，并给出性能优化的措施。

- 后端性能测试工具和 GUI 自动化测试工具最大的区别是什么？

    - 模拟用户行为的方式不同

        GUI 自动化测试工具模拟的是用户的行为，记录的是用户对界面上控件的操作。性能测试工具模拟的是客户端和服务端之间的通信协议和数据。

    - 测试的执行方式不同

        GUI 自动化测试一般是单用户执行验证；性能测试往往需要同时模拟大量的并发用户。

- 后端性能测试工具的原理是什么？

    - 后端性能测试工具会**基于客户端和服务器的通信协议**，**构建模拟业务操作的虚拟用户脚本**。

        LoaRunner 是通过**录制后再修改**的方式生成虚拟用户脚本；JMeter 主要是通过**添加各种组件**，然后**对组件进行配置**的方式生成虚拟用户脚本。

    - 开发完成虚拟用户脚本之后，后端性能测试工具会以**多线程或多进程的方式并发执行**虚拟用户脚本，来模拟大量并发用户的同时访问，从而对服务器施加测试负载。

        发起测试负载的机器称为压力产生器，压力控制器主要用来统一管理与协调这些压力产生器。

    - 在施加测试负载的整个过程中，后端性能测试工具除了需要监控和收集被测系统的各种性能数据之外，还需要监控被测系统各个服务器的各种软硬件资源。

    - 测试完成之后，后端性能测试工具会将系统监控器收集的所有信息汇总成完整的测试报告。

- 后端性能测试场景设计有哪些内容？

    性能场景设计，目的是描述性能测试过程中所有与测试负载以及监控相关的内容，通常来说，主要涉及以下部分：

    - 测试负载组成
        - 虚拟用户脚本
        - 各个虚拟用户脚本的并发数量
        - 总的并发数量
    - 负载策略
        - 加压策略
        - 减压策略
        - 最大负载运行时间
        - 延时策略
    - 资源监控范围定义
        - 操作系统级别的监控指标
        - 应用服务器的监控指标
        - 数据库服务器的监控指标
        - 缓存集群的监控指标
    - 终止方式
        - 脚本出错时的处理方式
        - 负载熔断机制
    - 负载产生规划
        - 压力产生器的梳理
        - 网络带宽的要求

- 主流的后端性能测试工具

    LoadRunner、JMeter、NeoLoad

### 31 | 前端性能测试工具原理与常用工具

⭐⭐ WebPagetest 工具的使用和测试报告结果中的重要数据

![](https://static001.geekbang.org/resource/image/6d/bd/6d886a2474f9dee528ee825574e601bd.png)

### 32 | 基于 LoadRunner 实现企业级服务端性能测试

后端性能测试工具首先通过**虚拟用户脚本生成器**生成基于协议的**虚拟用户脚本**，然后根据性能测试场景设计的要求，通过**压力控制器**控制协调各个**压力产生器**以**并发**的方式执行虚拟用户脚本，并且在执行测试过程中，通过**系统监控器**收集各种性能指标以及系统资源占有率，最后通过**测试结果分析器**展示测试结果数据。

LoadRunner 中，Virtual UserGenerator 对应虚拟用户脚本生成器，Controller 对应压力控制器和系统监控器，Load Generator 对应压力产生器，Analysis 对应测试结果分析器。

LoadRunner 完成企业级后端性能测试的典型流程与步骤

![](https://static001.geekbang.org/resource/image/e8/94/e8e58ca14e80346be38291cf84bf2394.png)

- 制定负载计划

    主要包含以下内容

    - 系统整体的并发用户数。（高峰时期会有多少用户在上面）
    - 并发用户业务操作的分布情况。（有百分之几的用户在做什么操作（登录/搜索/订单））
    - 单一业务操作的用户行为模式。（两次操作之间停留了多久，完成同一业务的不同操作路径）
    - 并发用户高峰时期的时间分布规律。
    - 达到最高峰负载的时间长度。（从 0 到 10万花费了多久）

    可以采用 8/2 原则对高峰时段的用户负载进行建模。

- 录制并增强虚拟用户脚本

    - 识别被测应用使用的协议

        可以使用 Virtual User Generator 模块自带的 Protocol Advisor 识别被测应用使用的协议。

    - 录制脚本

        基本原理是，通过 GUI 界面对被测系统进行业务操作，Virtual User Generator 模块在后台捕获 GUI 操作所触发的客户端与服务端的所有交互，并产生基于 C 语言的虚拟用户脚本文件。

        需要明确了解哪些操作步骤会对服务端发起请求，要将这些操作步骤在虚拟用户脚本中**封装成 ”事务“**，封装的目的是**为了统计响应时间**。

        在录制脚本的过程中，建议**直接对发起后端调用的操作添加事务定义**，而不要等到脚本生成后再添加。

    - 完善录制得到的脚本

        - 在两个事务之间加入思考时间（Think Time）

            用户操作是会有等待的，不会快速频繁的执行操作；可能填写订单信息的时候需要一定时间才能提交。

            实际项目中，一般是先粗略估计一个值，然后在实际执行负载场景的过程中，再根据系统吞吐量调整。

        - 对界面输入的数据做参数化（Parameterization）操作

            性能测试脚本和测试数据分离

            事先建立性能测试的数据

        - 完成脚本的关联（Correlation）操作

            主要作用是，**取出前序调用返回结果中的某些动态值，传递给后续的调用**。

            LoadRunner 提供了功能强大的关联函数 **web_reg_save_param()**。这个关联函数支持多种动态值的获取方式，用得最多的是**基于“前序字符串匹配”加上“后续字符串匹配”的方式**。其中，字符串匹配，支持正则表达式。

        - 加入检查点（Check Point）

            类似功能测试中的断言。保证脚本按照原本设计的路径执行。最常用的检查点函数是 **web_reg_find()**，它的作用是通过**指定左右边界的方式“在页面中查找相应的内容”**。

    - 验证脚本的正确性

        - 以**单用户**的方式，在**有思考时间**的情况下执行脚本，确保脚本能够顺利执行，并且验证脚本行为以及执行结果是否正确；
        - 以**单用户**的方式，在**思考时间为零**的情况下执行脚本，确保脚本能够顺利执行，并且验证脚本行为以及执行结果是否正确；
        - 以**并发用户**的方式，在**有思考时间**的情况下执行脚本，确保脚本能够顺利执行，并且验证脚本行为以及执行结果是否正确；
        - 以**并发用户**的方式，在**思考时间为零**的情况下执行脚本，确保脚本能够顺利执行，并且验证脚本行为以及执行结果是否正确。

- 创建并定义性能测试场景

    基于 Controller 的图形化用户界面操作

- 执行性能测试场景

- 分析测试报告

### 33 | 企业级实际性能测试案例与经验分享

- 性能基准测试（Performance Benchmark Test）

    性能基准测试，会基于**固定的硬件环境和部署架构**，通过执行**固定的性能测试场景**得到系统的性能测试报告，然后与上一版本发布时的指标进行对比，如果发现指标有“恶化”的趋势，就需要进一步排查。

    - 同一事务的响应时间变慢了
    - 系统资源的占用率变高了
    - 网络带宽的使用量变高了

    最重要的就是要在**一致的环境**下测试。

- 稳定性测试（Stability Test）

    主要是通过长时间（7*24 小时）模拟被测系统的测试负载，来观察系统在长期运行过程中是否有潜在的问题。通过对系统指标的监控，稳定性测试可以发现诸如内存泄漏、资源非法占用等问题。

    一般是采用**“波浪式”的测试负载**，比如先逐渐加大测试负载，在高负载情况下持续 10 多个小时，然后再逐渐降低负载，这样就构成了一个“波浪”，整个稳定性测试将由很多个这样的波浪连续组成。

    稳定性测试成功完成的标志，主要有以下三项：

    - 系统资源的所有监控指标不存在“不可逆转”的上升趋势；
    - 事务的响应时间不存在逐渐变慢的趋势；
    - 事务的错误率不超过 1%。

- 并发测试

    并发测试，是在**高并发**情况下**验证单一业务功能**的正确性以及性能的测试手段。

    高并发测试一般使用**思考时间为零**的虚拟用户脚本来发起**具有“集合点”的测试**。

    并发测试，往往被当作功能测试的补充，主要用于发现诸如多线程、资源竞争、资源死锁之类的错误。要执行并发测试，就需要加入“集合点”，所以往往需要修改虚拟用户脚本。

- 容量规划测试

    容量规划的主要目的是，解决当**系统负载将要达到极限处理能力**时，我们应该如何通过垂直扩展（增加单机的硬件资源）和水平扩展（增加集群中的机器数量）增加系统整体的负载处理能力的问题。

    理论上讲，整个集群的处理能力将等于单台机器的处理能力乘以集群的机器数，但是实际情况并不是这样。**实际的集群整体处理能力一定小于这个值**，但具体小多少就是要靠实际的测试验证了。

    理想的状态是，集群整体的处理能力能够随着集群机器数量的增长呈线性增长。但是，随着机器数量的不断增长，总会在达到某个临界值之后，集群的整体处理能力不再继续呈线性增长。

## 测试数据准备

### 34 | 如何准备测试数据

- 基于 GUI 操作生成测试数据

    一般只用于手工测试，更重要的是它可以帮助找到创建一个测试数据的过程中，后端调用了哪些 API，以及修改了哪些数据库的业务表，是后面两种方法的基础。

    - 创建测试数据的效率非常低
    - 基于 GUI 的测试数据创建方法不适合封装成测试数据工具
    - 测试数据成功创建的概率不会太高
    - 会引入不必要的测试依赖（比如用户登录，依赖于用户注册）

- 通过 API 调用生成测试数据

    主流的测试数据生成方法。

    直接询问开发人员；直接阅读源代码；在独占的环境上执行 GUI 操作，监控服务器端的调用日志

    优点：可以保证创建的测试数据的准确性；测试数据准备的执行效率更高；把创建测试数据的 API 调用过程，封装成测试数据函数更方便；测试数据的创建可以完全依赖于 API 调用，方便更新。

    缺点：并不是所有的测试数据创建都有对应的 API 支持；会存在多个 API 调用之间传递数据，增加了测试数据准备函数的复杂性；对于需要批量创建海量数据的场景，还是会力不从心。

- 通过数据库操作生成测试数据

    主流的测试数据生成方法。

    将创建数据需要用到的 SQL 语句封装成一个个的测试数据准备函数，当我们需要创建数据时，直接调用这些封装好的函数即可。

- 综合运用 API 和数据库的方式生成数据

    最典型的应用场景是，先通过 API 调用生成基础的测试数据，然后使用数据库的 CRUD 操作生成符合特殊测试需求的数据。

### 35 | 测试数据的痛点

在测试用例执行过程中，创建所需的数据往往会耗时较长，从而使得测试用例执行的时间变长；

在测试执行之前，先批量生成所有需要用到的测试数据，就有可能出现在测试用例执行时，这些事先创建好的数据已经被修改而无法正常使用了的情况；

在微服务架构下，测试环境本身的不稳定，也会阻碍测试数据的顺利创建。

- On-the-fly（实时创建）

    数据都是由测试用例自己维护的，不会依赖于测试用例外的任何数据，从而**保证了数据的准确性和可控性**，最大程度地**避免了出现“脏”数据的可能**。

    弊端

    - 实时创建测试数据比较耗时
    - 测试数据本身存在复杂的关联性
    - 微服务架构的调整

- Out-of-box（事先创建）

    Out-of-box 最致命的问题是“脏”数据。

在实际的测试项目中，我们可以根据测试数据的特性，把它们分为两大类，用业内的行话来讲就是“死水数据”和“活水数据”。

“死水数据”是指那些相对稳定，不会在使用过程中改变状态，并且可以被多次使用的数据。(out-of-box)

“活水数据”是指那些只能被一次性使用，或者经常会被修改的测试数据。(on-the-fly)

### 36 | 统一测试数据平台

- 测试数据准备的 1.0 时代

    这个阶段典型的方法就是，将**测试数据准备的相关操作封装成数据准备函数**。

    ```java
    public static User createUser(String userName, String password, UserType userType, PaymentDetail paymentDetail, Country country, boolean enable2FA){ 
        //使用API调用的方式和数据库CRUD的方式实际创建测试数据 ...
    }
    ```

    最大的短板在于：参数非常多，也很复杂。

    由于绝大多数测试场景不可能用到所有的参数，所以可以将其分解。

    ```java
    createUserImpl(A, B, C, D, E){
        //使用API调用的方式和数据库CRUD的方式实际创建测试数据 ...
    }
    
    createDefaultUser() {
        createUserImpl(A, B, C, D, E);
    }
    
    createXXXUser(A) {
        createUserImpl(A, B, C, D, E);
    }
    ```

    这种方式也有问题：对于参数比较多的情况，会面临需要封装的函数也很多；底层函数参数发生变化时，需要修改所有的封装函数；数据准备函数的 JAR 包版本升级比较频繁。

- 测试数据准备的 2.0 时代

    builder pattern

    问题是：如果测试用例不是基于 Java 的，调用 Java 方法不是很方便。（跨平台使用问题）

- 测试数据准备的 3.0 时代

    基于 Java 开发的数据准备函数用 Spring Boot 包装成 Restful API，并结合 Swagger 给这些 Restful API 提供 GUI 界面和文档。

    ![](https://static001.geekbang.org/resource/image/7d/0d/7d4cdac895834f96e777234a0f6db40d.png)

    - 引入了 Core Service 和一个内部数据库。其中，内部数据库用于存放创建的测试数据的元数据；Core Service 在内部数据库的支持下，提供数据质量和数量的管理机制。

    - 当一个测试数据被创建成功后，为了使得下次再要创建同类型的测试数据时可以更高效，Core Service 会自动在后台创建一个 Jenkins Job。这个 Jenkins Job 会再自动创建 100 条同类型的数据，并将创建成功的数据的 ID 保存到内部数据库，当下次再请求创建同类型数据时，这个统一测试数据平台就可以直接从内部数据库返回已经事先创建的数据。

        在一定程度上，这就相当于将原本的 On-the-fly 转变成了 Out-of-box，缩短整个测试用例的执行时间。当这个内部数据库中存放的 100 条数据被逐渐被使用，导致总量低于 20 条时，对应的 Jenkins Job 会自动把该类型的数据补足到 100 条。而这些操作对外都是透明的，完全不需要我们进行额外的操作。

## 测试基础架构

### 37 | 如何搭建 Selenium Grid

测试基础架构指的是，执行测试过程中用到的所有基础硬件设施和相关的软件设施。

通常包括，测试执行的机器；测试用例代码仓库；发起测试执行的 Jenkins Job；统一的测试执行平台；测试用例执行过程中依赖的测试服务，比如提供测试数据的统一测试数据平台、提供测试全局配置的配置服务、生成测试报告的服务等。

Selenium Grid 的架构

![](https://static001.geekbang.org/resource/image/30/bf/3077a24abcd93f063c7510fb81ccf9bf.png)

Selenium Grid 是一种可以**并发执行** GUI 测试用例的测试执行机的**集群环境**，采用 **HUB** 和 **Node** 模式。

Selenium Hub 用来管理各个 Selenium Node 的注册信息和状态信息，并且接收远程客户端代码的测试调用请求，并把请求命令转发给符合要求的 Selenium Node 执行。

- 传统 Selenium Grid 的搭建方法

    - 下载 jar 包

    - java -jar <jar 包> -hub，启动 Hub

    - java -jar <jar 包> -role node -hub http://<hub_ip>:4444/grid/register

    - 测试用例

         DesiredCapabilities capability = DesiredCapabilitys.firefox();

        Webdriver driver = new RemoteWebDriver(new URL("http://<hub_ip>:4444/wd/hug"), capability);

- 基于 Docker 的 Selenium Grid 的搭建方法

### 38 | 测试执行环境的架构设计

- 早期的测试基础架构

    测试用例存储在代码仓库中，Jenkins job 来 pull 代码并完成测试的发起工作。

    ![](https://static001.geekbang.org/resource/image/f2/ba/f20e2df431199038d25e9cabcacc31ba.png)

    流程：

    - 自动化测试开发人员在本地机器开发和调试测试用例；
    - 将开发的测试用例代码，Push 到代码仓库；
    - 在 Jenkins 中建立一个 Job，用于发起测试的执行。

    可能存在的问题：

    每次执行时候，需要先看测试执行机是否处于可用状态，当测试执行机的 IP 或者名字，数量变化时候，都需要提前知道这些信息。

- 经典的测试基础架构

    用 Selenium Grid 代替早期测试基础架构中的 ”远端或本地固定的测试执行机器“。

    ![](https://static001.geekbang.org/resource/image/8e/8c/8e9da1055a174071903c785fcabdf78c.png)

    可能存在的问题：

    随着测试用例的逐渐增多，Selenium Grid 方案在**集群扩容**、**集群 Node 维护**等方面遭遇到了瓶颈，并且 Jenkins Job 也因为测试用例的增多而变得臃肿不堪。

- 基于 Docker 实现的 Selenium Grid 测试基础架构

    ![](https://static001.geekbang.org/resource/image/56/75/56142f87923aa3c2e27746b91e2c0f75.png)

- 引入统一测试执行平台的测试基础架构

    测试用例可以实现版本化管理，测试用例的版本号与开发的版本号一致；

    提供基于 Restful API 的测试执行接口供 CI/CD 使用，任何时候通过一个标准的 Restful API 发起测试，CI/CD 流水线的脚本无须知道发起测试的命令行的具体细节，只要调用统一的 Restful API 即可。

    ![](https://static001.geekbang.org/resource/image/40/43/40be3cb1a925bbcf24c2a710f0711443.png)

- 基于 Jenkins 集群的测试基础架构

    ![](https://static001.geekbang.org/resource/image/db/f5/db974bd10bf6146c2ad1afbdb310ccf5.png)

- 测试负载自适应的测试基础架构

    为了解决  Selenium Grid 中 Node 的数量多少才合适的问题，诞生了 Selenium Grid 的自动扩容和收缩技术。

    核心思想是：通过单位时间内的测试用例数量，以及期望执行完成所有测试用例的时间，来动态计算需要多少 Node，如果需要多的化，再基于 Docker 快速添加 Node；空闲时间则去监控哪些 Node 没有被使用，回收之。

    ![](https://static001.geekbang.org/resource/image/da/a4/daeb20ac9fee266ecbabbfadfa2305a4.png)

### 39 | 大型全球化电商的测试基础架构设计

大型全球化电商网站全局测试基础架构的设计思路，可以总结为“测试服务化”。

测试过程中需要用的任何功能都通过服务的形式提供，每类服务完成一类特定功能，这些服务可以采用最适合自己的技术栈，独立开发，独立部署。

某种理想化的测试基础架构概念图

![](https://static001.geekbang.org/resource/image/d9/71/d9456825d8e9568e9453efe5207fb571.png)

- 统一测试执行服务

    这个统一测试执行服务采用的 Restful API 调用，主要用户就是 CI/CD 流水线脚本。

- 统一测试数据服务

    需要准备测试数据的，都可以通过 Restful API 调用统一测试数据服务，然后由它在被测系统中实际创建或者搜索符合要求的测试数据。

- 测试执行环境准备服务

    特指具体执行测试的测试执行机器集群。

- 被测系统部署服务

    调用 DevOps 团队的软件安装和部署脚本。

    被测系统部署服务，一般由 CI/CD 流水线脚本来调用。

- 测试报告服务

    测试报告服务的实现中引入了一个 NoSQL 数据库，用于存储结构各异的测试报告元数据。

    同时，由于各种测试报告的元数据都存在了这个 NoSQL 数据库中，所以我们就可以开发一些用于分析统计的 SQL 脚本，帮助我们获得质量相关信息的统计数据。

- 全局测试配置服务

    其本质是要解决测试配置和测试代码的耦合问题。

    可以把配置值从代码中抽离出去放到单独的配置文件中，然后代码通过读取配置文件的方式来动态获取配置值。

使用案例

- 首先，CI/CD 流水线脚本会以异步或者同步的方式调用被测系统部署服务，安装部署被测软件的正确版本。
- 其次，被测系统部署完成后，CI/CD 脚本就会调用统一测试执行服务。
- 接下来，统一测试执行服务会将测试用例的数量、浏览器的要求，以及需要执行完成的时间作为参数，调用测试执行环境准备服务。
- 然后，测试用例执行过程中，会依赖统一测试数据服务来准备测试需要用到的数据，并通过全局测试配置服务获取测试相关的配置与参数。
- 最后，在测试执行结束后，还会自动将测试报告以及测试报告的元数据发送给测试报告服务进行统一管理。

提升测试效能的服务：全局 Mock 服务，工程效能工具链仓库 etc。

## 测试新技术

### 40 | 探索式测试

> Exploratory software testing is a style of software testing that emphasizes the **personal freedom** and responsibility of the individual tester to **continually optimize the value of her work** by treating test-related learning, test design, test execution, and test result interpretation as mutually supportive activities that run in parallel throughout the project.

探索式测试是一种**软件测试风格**，而不是一种具体的软件测试技术。

探索式测试强调测试工程师的个人自由和责任，目的是**为了持续优化其工作的价值**。

探索式测试建议在整个项目过程中，将测试相关学习，测试设计，测试执行和测试结果解读作为相互支持的活动，并行执行。

探索式测试也是可以采用分层测试的策略。

- 首先，会对软件的单一功能进行比较细致的探索式测试。
- 然后，会开展系统交互的探索式测试，这个过程通常会采用基于反馈的探索式测试方法。

### 41 | 测试驱动开发（TDD）

TDD 是一种开发理念。它的核心思想，是在开发人员实现功能代码前，先设计好测试用例的代码，然后再根据测试用例的代码编写产品的功能代码，最终目的是让开发前设计的测试用例代码都能够顺利执行通过。

TDD 中通常会用到很多常见的自动化测试技术，使得测试在整个软件生命周期中的重要性和地位得到了大幅提升。

TDD 的优势

- 保证开发的功能一定是符合实际需求的。
- 更加灵活的迭代方式。
- 保证系统的可扩展性。
- 更好的质量保证。
- 测试用例即文档。

TDD 的实施过程

- 为需要实现的新功能添加一批测试；
- 运行所有测试，看看新添加的测试是否失败；
- 编写实现软件新功能的实现代码；
- 再次运行所有的测试，看是否有测试失败；
- 重构代码；
- 重复以上步骤直到所有测试通过。

TDD 的三个注意点

- 需要控制测试用例的粒度
- 注意代码的简洁和高效
- 通过重构保证代码的优雅和简洁

### 42 | 精准测试

精准测试，借助一些**高效的算法**和工具，**收集、可视化**并且分析原生的测试数据，从而建立起一套**测试分析系统**。

传统测试的主要短板

- 测试的维护成本日益升高
- 测试过程低效
- 缺乏有效的回归用例选取机制
- 测试结果的可信度不高
- 无论是白盒测试还是黑盒测试都有局限性

精准测试的主要特征

- 对传统测试的补充
- 采用的是白盒测试和黑盒测试相结合的模式
- 数据可信度高
- 不直接面对产品代码。通过算法和软件实现对测试数据和过程的采集，并不会直接面向代码，不依赖与产品代码。
- 与平台无关、多维度的测试分析算法系统

精准测试的具体方法

目前业界最成熟并且已经产品化的精准测试体系，来自于国内公司“星云测试”。《**星云精准测试白皮书**》

- 软件精准测试示波器

- 测试用例和被测产品代码的双向追溯。

    就是通过一定的技术手段实现测试用例和被测产品代码的双向关联。

    ![](https://static001.geekbang.org/resource/image/64/ef/644bcdeca4c82cf13d7a037c16ab6eef.png)

- 智能回归测试用例选取算法

    避免了人工选取回归测试用例时可能存在的测试盲点，也减少了执行回归测试的时间，同时还能够保证计算结果的精确性，大大降低了回归测试的风险。

- 测试用例的聚类分析

### 43 | 渗透测试

渗透测试指的是，由专业安全人员模拟黑客，**从其可能存在的位置对系统进行攻击测试**，在真正的黑客入侵前找到隐藏的安全漏洞，从而达到保护系统安全的目的。

- 有针对性的测试
- 外部测试
- 内部测试
- 盲测
- 双盲测试

执行渗透测试的步骤

- 规划和侦察

    定义测试的范围和目标、初步确定要使用的工具和方法、明确需要收集的情报（例如，网络和域名，邮件服务器），以更好地了解目标的工作方式及其潜在的安全漏洞。

- 安全扫描

    - 静态分析阶段，是通过**扫描所有代码来估计其运行时的方式**。这里，我们可以借助一些工具来一次性地扫描所有代码。目前，主流工具有 **Fortify SCA** 和 **Checkmarx Suite**。
    - 动态分析阶段，则是在**代码运行时进行扫描**。这样的扫描更能真实反映程序的行为，可以实时提供应用程序的运行时视图，比静态扫描更准确、实用。

- 获取访问权限

    在这一步，测试人员将模拟黑客对应用程序进行网络攻击，例如使用 SQL 注入或者 XSS 跨站脚本攻击等，以发现系统漏洞。

- 维持访问权限

    这个阶段的目的是，查看被发现的漏洞**是否可以长期存在于系统中**。

- 入侵分析

    通常情况下，我们需要将测试结果汇总成一份详尽的测试报告，并详细说明：

    可以被利用的特定漏洞；

    利用该漏洞的具体步骤；

    能够被访问的敏感数据；

    渗透测试人员能够在系统中不被侦测到的存在时间。

    专业的安全人员会分析这些信息，以指导和帮助我们配置企业的 WAF(Web Application Firewall)，同时提供对其他应用程序的安全解决方案，以修补安全漏洞并防范未来的恶意攻击。

渗透测试常用工具

- Nmap

    Nmap 是进行**主机检测**和**网络扫描**的重要工具。它不仅可以收集信息，还可以进行漏洞探测和安全扫描，从主机发现、端口扫描到操作系统检测和 IDS 规避 / 欺骗。

- Aircrack-ng

    Aircrack-ng 是**评估 Wi-Fi 网络安全性**的一整套工具。它侧重于 Wi-Fi 安全的领域，主要功能有：网络侦测、数据包嗅探、WEP 和 WPA/WPA2-PSK 破解。

- sqlmap

    sqlmap 是一种开源的基于命令行的渗透测试工具。它能够自动进行 SQL 注入和数据库接入，并且支持所有常见并广泛使用的数据库平台。

- Wifiphisher

    Wifiphisher 是一种恶意接入点工具，可以对 Wi-Fi 网络进行自动钓鱼攻击。

- AppScan

    AppScan 是 IBM 公司的一款**企业级商业 Web 应用安全测试工具**，采用的是黑盒测试，可以扫描常见的 Web 应用安全漏洞。

    - 首先，从起始页爬取站下所有的可见页面，同时测试常见的管理后台；
    - 然后，利用 SQL 注入原理测试所有可见页面，是否在注入点和跨站脚本攻击的可能；
    - 同时，检测 Cookie 管理、会话周期等常见的 Web 安全漏洞。

### 44 | 基于模型的测试

基于模型的测试，即 Model-Based-Testing，简称 MBT。

MBT，是自动化测试的一个分支。它是**将测试用例的设计依托于被测系统的模型**，并**基于该模型自动生成测试用例的技术**。其中，这个被测系统的模型表示了被测系统行为的预期，也可以说是代表了我们对被测系统的预期。

MBT 的基本原理是通过建立被测系统的设计模型，然后结合不同的算法和策略来遍历该模型，以此生成测试用例的设计。

![](https://static001.geekbang.org/resource/image/08/f9/08c3605390ca20c46b63e92f29f375f9.png)

开发者首先根据产品需求或者说明来构建模型，然后结合测试对象生成测试用例，测试用例针对测试对象执行完之后，生成测试报告比对测试结果。

常用模型简介

- 有限状态机
- 状态图
- UML

MBT 工具

- BPM-X
- fMBT
- GraphWalker

MBT 的优势

- 测试用例的维护更轻松。
- 软件缺陷发现得更早。
- 测试自动化的水平更高。
- 测试覆盖率变得更高，使得彻底的测试（即：穷尽测试）成为了可能。
- 基于模型间接维护测试用例的方式更高效。

MBT 的劣势

- 学习成本较高。
- 使用 MBT 的初期投资较大。
- 早期根据模型生成测试用例的技术并不是非常成熟。

## 互联网架构核心知识

对加了消息队列的系统进行测试；对缓存的了解；

### 45 | 网站高性能架构设计

- 前端性能

    本质上来说就是让用户觉得打开页面时间越短越好。**雅虎前端性能优化团队的 35 条原则**。

- 后端服务器相关的性能优化和架构设计

    **采用缓存**

    - 浏览器级别的缓存，会存储在前在网络上下载的静态资源；
    - CDN 属于部署在网络服务供应商机房中的缓存；
    - 反向代理服务器，属于用户数据中心最前端的缓存；
    - 数据库中的热点数据；
    - DNS 服务器，为了减少重复查询的次数也采用缓存。

    缓存主要用来**存储那些相对变化较少，并且遵从“二八原则”的数据**。这里的“二八原则”指的是 80% 的数据访问会集中在 20% 的数据上。

    缓存技术并**不适用于那些需要频繁修改**的数据。

    为了解决缓存集群故障这个问题，有些网站会使用**缓存热备**等技术手段来提供缓存的高可用性。

    分布式缓存架构的主流技术方案：

    - JBoss Cache
    - Memcached

    从测试的角度看，需要考虑哪些与缓存相关的测试场景：

    - 对于前端的测试场景，需要分别考虑**缓存命中**和**缓存不命中**情况下的页面加载时间。
    - 基于**缓存过期**测试策略的设计，需要考虑到必须要重新获取数据的测试场景。
    - 需要针对可能存在的**缓存“脏数据”**，进行有针对性的测试。缓存“脏数据”，是指数据库中的数据已经更新，但是缓存中的数据还没来得及更新的场景。
    - 需要针对可能的**缓存穿透**进行必要的测试。缓存穿透，是指访问的数据并不存在，所以这部分数据永远不会有被缓存的机会，因此此类请求会一直重复访问数据库。
    - 系统冷启动后，在**缓存预热阶段**的**数据库访问压力**是否会超过数据库实际可以承载的压力。
    - 对于分布式缓存集群来说，由于各集群使用的**缓存算法**不同，那么如果要在缓存集群中增加更多节点进行扩容的话，扩容对原本已经缓存数据的影响也会不同。所以，我们需要针对缓存集群扩容的场景，进行必要的测试和性能评估。

    **集群**

    从测试的角度看，需要考虑哪些与集群相关的测试场景：

    - 集群容量扩展。也就是说，集群中加入新的节点后，是否会对原有的 session 产生影响。
    - 对于无状态应用，是否可以实现灵活的实效转移。
    - 对于基于 session 的有状态应用，需要根据不同的 session 机制验证会话是否可以正常保持，即保证同一 session 始终都有同一个确定的节点在处理。
    - 当集群中的一个或者多个节点宕机时，对在线用户的影响是否符合设计预期。
    - 对于无状态应用来说，系统吞吐量是否能够随着集群中节点的数量呈线性增长。
    - 负载均衡算法的实际效果，是否符合预期。
    - 高并发场景下，集群能够承载的最大容量。

### 46 | 网站高可用架构设计

- 从硬件层面加入必要的冗余

- 灰度发布

    假定现在有一个包含 100 个节点的集群需要升级安装新的应用版本，那么这个时候的更新过程应该是：

    - 首先，从负载均衡器的服务器列表中删除其中的一个节点；
    - 然后，将新版本的应用部署到这台删除的节点中并重启该服务；
    - 重启完成后，将包含新版本应用的节点重新挂载到负载均衡服务器中，让其真正接受外部流量，并严密观察新版本应用的行为；
    - 如果没有问题，那么将会重复以上步骤将下一个节点升级成新版本应用。如果有问题，就会回滚这个节点的上一个版本。
    - 如此反复，直至集群中这 100 个节点全部更新为新版本应用。

- 加强应用上线前的测试，或者开启预发布验证

    引入预发布服务器消除测试环境和生产环境的差异。

### 47 | 网站伸缩性架构设计

可伸缩性翻译自 Scalability，指的是通过简单地增加硬件配置而使服务处理能力呈线性增长的能力。

- 根据功能进行物理分离来实现伸缩

    - “横切”，从上到下分成不同的层
    - “纵切”，按照不同的功能模块进行划分

- 物理分离后的单一功能通过增加或者减少硬件来实现伸缩

    - 增加单一服务器的硬件资源，CPU，内存，硬盘等

    - 多台机器组成集群来共同负担并发压力。

        基于集群的可伸缩性设计，是和网站本身的分层架构设计相对应的：

        - 在应用服务器层面有应用服务器集群的可伸缩性架构设计；

            为了保证这批服务器对外暴露的是一个统一的节点，我们就需要一个**负载均衡器**作为统一的窗口来对外提供服务，同时负载均衡器会把实际的业务请求转发给集群中的机器去具体执行。

            从测试的角度看，需要考虑哪些与集群相关的测试场景：

            - 需要通过压力测试来得出单一节点的负载承受能力；

            - 验证系统整体的负载承受能力，是否能够随着集群中的节点数量呈现线性增长；
            - 集群中节点的数量是否有上限；
            - 新加入的节点是否可以提供和原来节点无差异的服务；
            - 对于有状态的应用，是否能够实现一次会话（session）的多次请求都被分配到集群中某一台固定的服务器上；
            - 验证负载均衡算法的准确性。

        - 在缓存服务器层面有缓存服务器的可伸缩性架构设计；

            采用 **Hash 一致性算法**。巧妙地解决缓存集群的扩容问题，保证了新增机器节点的时候大部分的缓存不会失效。

            从测试的角度看，需要考虑哪些与缓存服务器相关的测试场景：

            - 针对缓存集群中新增节点的测试，验证其对原有缓存的影响是否足够小；
            - 验证系统冷启动完成后，缓存中还没有任何数据的时候，如果此时网站负载较大，数据库是否可以承受这样的压力；
            - 需要验证各种情况下，缓存数据和数据库数据的一致性；
            - 验证是否已经对潜在的缓存穿透攻击进行了处理，因为如果有人刻意利用这个漏洞来发起海量请求的话，就有可能会拖垮数据库。

        - 在数据库层面有数据库服务器的可伸缩性架构设计。

            - 业务分库。从业务上将一个庞大的数据库拆分成多个不同的数据库。
            - 读写分离的数据库设计。其中主库用于所有的写操作，从库用于所有的读操作，然后主从库会自动进行数据同步操作。
            - 分布式数据库。
            - NoSQL 设计。

            从测试的角度看，需要考虑哪些与数据库相关的测试场景：

            - 正确读取到刚写入数据的延迟时间；

            - 在数据库架构发生改变，或者同样的架构数据库参数发生了改变时，数据库基准性能是否会发生明显的变化；

            - 压力测试过程中，数据库服务器的各项监控指标是否符合预期；

            - 数据库在线扩容过程中对业务的影响程度；

            - 数据库集群中，某个节点由于硬件故障对业务的影响程度。

### 48 | 网站可扩展性架构设计

可扩展性翻译自 Extensibility，指的是**网站的架构设计能够快速适应需求的变化**，当需要增加新的功能实现时，对原有架构不需要做修改或者做很少的修改就能够快速满足新的业务需求。

提升网站可扩展性性的核心，就是**降低系统各个模块和组件之间的耦合**。

- 微服务架构

    在微服务架构下，一个大型复杂软件系统不再由一个单体组成，而是由一系列的微服务组成。其中每个微服务可被独立开发和部署，各个微服务之间是松耦合的。每个微服务仅专注于完成一件任务，并要很好地完成该任务。

- 事件驱动架构

    事件驱动架构的落地靠的是**消息队列**。

    事件驱动架构最典型的一个应用就是操作系统中常见的**生产者和消费者模式**，将其应用到网站设计中就是**分布式消息队列**。

    其基本核心原理是各模块之间不存在直接的调用关系，而是使用消息队列，通过生产者和消费者模式来实现模块间的协作，从而保持模块与模块间的松耦合关系。

    引入消息队列后，测试人员需要额外关注的点：

    - 从**构建测试数据**的角度来看，为了以解耦的方式测试系统的各个模块，我们就需要**在消息队列中构造测试数据**。
    - 从**测试验证**的角度来看，我们不仅需要验证模块的行为，还要**验证模块在消息队列中的输出是否符合预期**。
    - 从**测试设计**的角度来看，我们需要**考虑消息队列满**、**消息队列扩容**等情况下系统功能是否符合设计预期。
    - **某台消息队列服务器宕机**的情况下，丢失消息的**可恢复性**以及新的消息不会继续发往宕机的服务器等等。

